{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a common act in general to try and reduce one's environmental impact or carbon footprint. Company's on the other hand offer services and products that are environmentally friendly and sustainable and in turn, make it part of their ideals and values. Companies, therefore, are highly interested in determining and knowing whether people at large believe climate change is a real threat or not.  Companies will then use the information in their respective market research in efforts to gauge opinions on how their products or services are received.\n",
    "\n",
    "### With that being said, EDSA provided a challenge where I am required to create a machine learning model that is able to classify whether an individual believes climate change is real or not based on their previous tweet(s).\n",
    "\n",
    "### The information as a whole will also help companies in future marketing strategy, increasing insights about how their services are perceived and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's not like we lack evidence of anthropogenic global warming\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking care of null values - Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15819"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking if an entry is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "message      0\n",
       "tweetid      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking if a string in an entry is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = []\n",
    "for i, sentiment, message, tweetid in df.itertuples():\n",
    "    if type(message) == 'str':\n",
    "        if message.issspace():\n",
    "            blank.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking care of null values - Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking if an entry is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    0\n",
       "tweetid    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking if a string in an is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_t = []\n",
    "for i, message, tweetid in df_test.itertuples():\n",
    "    if type(message) == 'str':\n",
    "        if message.issspace():\n",
    "            blank_t.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text\n",
    "\n",
    "#### To clean up the text. I will be using a combination of regular expression to remove unwanted features, Tweettokenizer to tokenize the text and will also lemmatize the text in a single fucntion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating relevant preprocessing instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "token = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(Data):\n",
    "    tweet_list = []\n",
    "\n",
    "    for tweet in Data:\n",
    "    \n",
    "        # converting text to lowercase\n",
    "        doc = tweet.lower()\n",
    "    \n",
    "        # remove all punctuation and special characters from a tweet\n",
    "        doc = re.sub(r'\\W', ' ', doc)\n",
    "    \n",
    "        # remove all numbers\n",
    "    \n",
    "        doc = re.sub(r'\\d', ' ', doc)\n",
    "\n",
    "        # remove all singe characters after special characters have been removed\n",
    "        doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n",
    "    \n",
    "        # remove all single characters from the start\n",
    "        doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc)\n",
    "    \n",
    "        # substituting multiple spaces with a single space\n",
    "        doc = re.sub(r'\\s+', ' ', doc)\n",
    "    \n",
    "        # Tokenizing and lemmatization\n",
    "    \n",
    "        doc = [lem.lemmatize(word) for word in token.tokenize(doc)]\n",
    "    \n",
    "        # joining to get the tokens back into a string\n",
    "        doc = ' '.join(doc)\n",
    "    \n",
    "        # appending to list\n",
    "    \n",
    "        tweet_list.append(doc)\n",
    "    \n",
    "    \n",
    "    return tweet_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = cleaning_text(df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['polyscimajor epa chief doesn think carbon dioxide is main cause of global warming and wait what http co yelvcefxkc via mashable',\n",
       " 'it not like we lack evidence of anthropogenic global warming',\n",
       " 'rt rawstory researcher say we have three year to act on climate change before it too late http co wdt kdur http co anpt',\n",
       " 'todayinmaker wired wa pivotal year in the war on climate change http co wotxtlcd',\n",
       " 'rt soynoviodetodas it and racist sexist climate change denying bigot is leading in the poll electionnight']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_clean'] = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief doesn think carbon diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>it not like we lack evidence of anthropogenic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt rawstory researcher say we have three year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired wa pivotal year in the war ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt soynoviodetodas it and racist sexist climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15814</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @ezlusztig: They took down the material on ...</td>\n",
       "      <td>22001</td>\n",
       "      <td>rt ezlusztig they took down the material on gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15815</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @washingtonpost: How climate change could b...</td>\n",
       "      <td>17856</td>\n",
       "      <td>rt washingtonpost how climate change could be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15816</td>\n",
       "      <td>0</td>\n",
       "      <td>notiven: RT: nytimesworld :What does Trump act...</td>\n",
       "      <td>384248</td>\n",
       "      <td>notiven rt nytimesworld what doe trump actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15817</td>\n",
       "      <td>-1</td>\n",
       "      <td>RT @sara8smiles: Hey liberals the climate chan...</td>\n",
       "      <td>819732</td>\n",
       "      <td>rt sara smile hey liberal the climate change c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15818</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @Chet_Cannon: .@kurteichenwald's 'climate c...</td>\n",
       "      <td>806319</td>\n",
       "      <td>rt chet_cannon kurteichenwald climate change e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid  \\\n",
       "0              1  PolySciMajor EPA chief doesn't think carbon di...   625221   \n",
       "1              1  It's not like we lack evidence of anthropogeni...   126103   \n",
       "2              2  RT @RawStory: Researchers say we have three ye...   698562   \n",
       "3              1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736   \n",
       "4              1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954   \n",
       "...          ...                                                ...      ...   \n",
       "15814          1  RT @ezlusztig: They took down the material on ...    22001   \n",
       "15815          2  RT @washingtonpost: How climate change could b...    17856   \n",
       "15816          0  notiven: RT: nytimesworld :What does Trump act...   384248   \n",
       "15817         -1  RT @sara8smiles: Hey liberals the climate chan...   819732   \n",
       "15818          0  RT @Chet_Cannon: .@kurteichenwald's 'climate c...   806319   \n",
       "\n",
       "                                           message_clean  \n",
       "0      polyscimajor epa chief doesn think carbon diox...  \n",
       "1      it not like we lack evidence of anthropogenic ...  \n",
       "2      rt rawstory researcher say we have three year ...  \n",
       "3      todayinmaker wired wa pivotal year in the war ...  \n",
       "4      rt soynoviodetodas it and racist sexist climat...  \n",
       "...                                                  ...  \n",
       "15814  rt ezlusztig they took down the material on gl...  \n",
       "15815  rt washingtonpost how climate change could be ...  \n",
       "15816  notiven rt nytimesworld what doe trump actuall...  \n",
       "15817  rt sara smile hey liberal the climate change c...  \n",
       "15818  rt chet_cannon kurteichenwald climate change e...  \n",
       "\n",
       "[15819 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting train data into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['message_clean']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 -  Cross validation of our train data set\n",
    "\n",
    "### Using the train data set, I perfom cross validation in building a classification model via a pipeline object and training it with the data. The built model will then be used to predict the results of the full test data set in section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the train data into train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building pipeline object to vectorize the cross validation train test split and train the model.\n",
    "\n",
    "### I have built 7 classification models with relevant hyper-parameter tuning. \n",
    "\n",
    "### To speed up the process of obtaining the macro f1_score of all the models, i built a pipeline object function to easily train the model and vectiorised the text. Further more, I looped through all the models to train them, predict the y_train of the train test split and obtain their f1_score.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing classification models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pipeline object has been built inside a function called: pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_object(model):\n",
    "\n",
    "    text_clf = Pipeline([('tfid_vectorizer', TfidfVectorizer(ngram_range = (1,2), stop_words = stopwords.words('english'))), \n",
    "                     ('L_SVC', model)])\n",
    "    \n",
    "    return text_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building/creating models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC_model = pipeline_object(LinearSVC(multi_class = 'crammer_singer', C = 2.1, loss = 'hinge'))\n",
    "SVC_model = pipeline_object(SVC(kernel = 'linear', C = 0.1))\n",
    "Random_forest_model = pipeline_object(RandomForestClassifier(max_depth = 5, n_estimators = 10, max_features = 1))\n",
    "Ada_Boost_model = pipeline_object(AdaBoostClassifier())\n",
    "Multinomial_model = pipeline_object(MultinomialNB())\n",
    "Nearest_neighbors_model = pipeline_object(KNeighborsClassifier(3))\n",
    "Decision_tree_model = pipeline_object(DecisionTreeClassifier(max_depth = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting models into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LinearSVC_model, \n",
    "               SVC_model, \n",
    "               Random_forest_model,\n",
    "               Ada_Boost_model,\n",
    "              Multinomial_model,\n",
    "              Nearest_neighbors_model,\n",
    "              Decision_tree_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the names of the classification models in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['LinearSVC', 'SVC', 'Random forest', 'AdaBoost', 'MultinomialNB', 'Nearest Neighbors', 'Decision Tree']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the pipeline object model for all imported models with the train set of the cross validation train test split and predict the result of the respective test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LinearSVC model to the CV train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the CV test set\n",
      "Obtaining f1_score\n",
      "\n",
      "\n",
      "Fitting SVC model to the CV train set\n",
      "Predicting the CV test set\n",
      "Obtaining f1_score\n",
      "\n",
      "\n",
      "Fitting Random forest model to the CV train set\n",
      "Predicting the CV test set\n",
      "Obtaining f1_score\n",
      "\n",
      "\n",
      "Fitting AdaBoost model to the CV train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the CV test set\n",
      "Obtaining f1_score\n",
      "\n",
      "\n",
      "Fitting MultinomialNB model to the CV train set\n",
      "Predicting the CV test set\n",
      "Obtaining f1_score\n",
      "\n",
      "\n",
      "Fitting Nearest Neighbors model to the CV train set\n",
      "Predicting the CV test set\n",
      "Obtaining f1_score\n",
      "\n",
      "\n",
      "Fitting Decision Tree model to the CV train set\n",
      "Predicting the CV test set\n",
      "Obtaining f1_score\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in zip(names, classifiers):\n",
    "    print(f'Fitting {name} model to the CV train set')\n",
    "    trained_model = model.fit(X_train, y_train)\n",
    "    \n",
    "    print('Predicting the CV test set')\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print('Obtaining f1_score\\n\\n')\n",
    "    f1_score_ = f1_score(y_test, y_pred, average = 'macro') \n",
    "    results.append([name, f1_score_])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LinearSVC', 0.6691529740819062],\n",
       " ['SVC', 0.31358461233553997],\n",
       " ['Random forest', 0.17687908496732027],\n",
       " ['AdaBoost', 0.5008575354153117],\n",
       " ['MultinomialNB', 0.34833604160863396],\n",
       " ['Nearest Neighbors', 0.5641407897459046],\n",
       " ['Decision Tree', 0.3717057333298502]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting results into a Dataframe to make it more readable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = dict(results)\n",
    "results_series = pd.Series(results_dict)    \n",
    "results_df = pd.DataFrame(results_series, columns = ['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.669153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.564141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.500858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.371706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.348336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SVC</td>\n",
       "      <td>0.313585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.176879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   f1_score\n",
       "LinearSVC          0.669153\n",
       "Nearest Neighbors  0.564141\n",
       "AdaBoost           0.500858\n",
       "Decision Tree      0.371706\n",
       "MultinomialNB      0.348336\n",
       "SVC                0.313585\n",
       "Random forest      0.176879"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the \"LinearSVC\" model performed best with the highest f1_score out of all the trained models. With that being said, I will now continue this challenge with the LinearSVC model as my chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Evaluation of the chosen Linear SVC model. \n",
    "\n",
    "## I will now retrain the model (built with the pipeline object) with the cross validation train test split, and use it to the predict the result of the full test data set in section 4...This step is merely to evaluate the Linear SVC model, and show it's performance in terms of Accuracy, classification report and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the pipleline LinearSVC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC_clf = pipeline_object(LinearSVC(multi_class = 'crammer_singer', C = 2.1, loss = 'hinge'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfid_vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourse...\n",
       "                                             'it', \"it's\", 'its', 'itself', ...],\n",
       "                                 strip_accents=None, sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('L_SVC',\n",
       "                 LinearSVC(C=2.1, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000,\n",
       "                           multi_class='crammer_singer', penalty='l2',\n",
       "                           random_state=None, tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearSVC_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the test set of the train test split (bear in mind we already know the f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LinearSVC = LinearSVC_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "#### (bear in mind that this macro F1_score is different from that of final score on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score: We know the score, however, I will print it again:\n",
      "\n",
      " 0.6691529740819062\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.54      0.63       401\n",
      "           0       0.63      0.40      0.49       666\n",
      "           1       0.79      0.83      0.81      2598\n",
      "           2       0.68      0.83      0.75      1081\n",
      "\n",
      "    accuracy                           0.74      4746\n",
      "   macro avg       0.71      0.65      0.67      4746\n",
      "weighted avg       0.74      0.74      0.74      4746\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "      -1    0     1    2\n",
      "-1  218   48   111   24\n",
      " 0   33  268   277   88\n",
      " 1   38  102  2148  310\n",
      " 2    4    9   170  898\n",
      "\n",
      "\n",
      "Accuracy:\n",
      "\n",
      " 0.7442056468605142\n"
     ]
    }
   ],
   "source": [
    "print('\\nf1_score: We know the score, however, I will print it again:\\n\\n', f1_score(y_test, y_pred_LinearSVC, average='macro'))\n",
    "print('\\n\\nClassification report:\\n\\n', classification_report(y_test, y_pred_LinearSVC))\n",
    "print('\\n\\nConfusion matrix:\\n\\n',pd.DataFrame(confusion_matrix(y_test, y_pred_LinearSVC), index = [-1, 0, 1, 2], columns = [-1, 0, 1, 2]))\n",
    "print('\\n\\nAccuracy:\\n\\n',accuracy_score(y_test, y_pred_LinearSVC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation of the test set\n",
    "\n",
    "### I will now be evaluating the full test data set by cleaning the data and then predicting the results using our test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing our test set using the cleaning text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = cleaning_text(df_test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['europe will now be looking to china to make sure that it is not alone in fighting climate change http co t rcgwdq',\n",
       " 'combine this with the polling of staffer re climate change and woman right and you have fascist state http co ifrm eexpj',\n",
       " 'the scary unimpeachable evidence that climate change is already here http co yaedqcv ki itstimetochange climatechange zeroco _',\n",
       " 'karoli morgfair osborneink dailykos putin got to you too jill trump doesn believe in climate change at all think it s hoax',\n",
       " 'rt fakewillmoore female orgasm cause global warming sarcastic republican']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of the test set with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pred = LinearSVC_clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = test_set_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  sentiment\n",
       "0  Europe will now be looking to China to make su...   169760          1\n",
       "1  Combine this with the polling of staffers re c...    35326          1\n",
       "2  The scary, unimpeachable evidence that climate...   224985          1\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263          1\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928          0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission to csv file that is to be uploaded/submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['tweetid', 'sentiment']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
